{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note when making models, we should always do some validation, be it cross validation or train-test split.  At times, the homework will ask you to build models without performing this task.  The goal of this is to get you used to using skleaern functionality, and not building robust models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Data\n",
    "* We will be working with 2 datasets, both of which are built into sklearn:\n",
    "* Boston Housing\n",
    "    * we will use this for prediction problems\n",
    "    * grab the boston dataset using load_boston()\n",
    "    * we will work with RAD, ZN, CRIM, AGE as features\n",
    "* Iris\n",
    "    * we will use this for classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "* make a class KNN that has methods to\n",
    "    * init\n",
    "        * should take k and distance measure\n",
    "        * should accept 2 possible distance measures (euclidean and city block)\n",
    "        * use error handling to check a proper distance measure is chosen and an odd k\n",
    "        * in practice, we can pick even numbered Ks for nearest neighbor, but stick with odd so our voting won't have ties.\n",
    "    * generate a distance matrix\n",
    "        * you can use cdist from scipy or sklearn pairwise distances\n",
    "    * get nearest neighbors\n",
    "        * give the user the option to pass in a distance matrix or raw data (like our feature data x)\n",
    "        * you can return the index value of the nearest neighbors, similar to how argsort works.\n",
    "        * your output should be a row for each observation and a column for each nearest neighbor\n",
    "        * don't worry about ties\n",
    "    * get prediction\n",
    "        * should support regression and classification\n",
    "        * if classification, do a majority vote\n",
    "        * if regression, do a mean of the nearest neighbors\n",
    "* remember, limit for loops, make use of the matrix functions, argmax and min, argsort etc.  \n",
    "    * be aware of how numpy handles ties, see documenation below, but for our purporses you can forgo worrying about the ties but be aware of how the sorting works\n",
    "* run each method in your class on the iris dataset, showing the first row of each returned matrix\n",
    "* make sure your methods are returing something and assigning attribuets using self (as needed)\n",
    "* https://numpy.org/doc/1.18/reference/generated/numpy.sort.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "* make a generator that\n",
    "    * takes each sentence and splits on the space (\" \")\n",
    "    * make sure to strip items (remove trailing and leading white space).  .strip(), .lstrip() and .rstrip() can achieve this\n",
    "    * get rid of punctuation\n",
    "    * as you generate documents make 4 dictionaries\n",
    "        * an index to word where the keys are an index value (starting at 0) and the values are words\n",
    "        * word to index dict (converse of the above dict)\n",
    "        * a document to list of tokens (keys are document ID and the values is a list of index values corresponding to each word in thee document)\n",
    "        * a count dict where keys are words and values are counts in all documents\n",
    "    * don't store punctuation or white space as keys in the dictionary\n",
    "    * make sure to iterate the generator itself, not a list\n",
    "    * note, you can use a double for loop her.  For documenet in corpus: for token in document, but make sure you're iterating the generator and not a list\n",
    "    * print out the dictionaries at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"this one time at band camp...\", # each of these is a document\n",
    "    \"this one time i went to band camp\", # each of these is a document\n",
    "    \"one time while at basketball camp\", # each of these is a document\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample output\n",
    "#\n",
    "#\n",
    "# corpus = [\n",
    "#    \"this is document 1\",\n",
    "#    \"this is document 2\"\n",
    "#]\n",
    "\n",
    "#idx_to_word = {\n",
    "#   0:\"this\",\n",
    "#   1:\"is\",\n",
    "#   2: \"document\",\n",
    "#   3: \"1\",\n",
    "#   4: \"2\"   \n",
    "#}\n",
    "\n",
    "#docs = {\n",
    "#    1:[0,1,2,3],\n",
    "#    2:[0,1,2,4]\n",
    "#}\n",
    "\n",
    "#word_count = {\n",
    "#   \"this\":2,\n",
    "#   \"is\":2,\n",
    "#   \"document\":2,\n",
    "#   \"1\":1,\n",
    "#   \"2\":1   \n",
    "#}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "* make your own classification grid search (by this we mean, take some x and y data, run different models, get a validation score for each, and return the best one).  For model in models: make mode;, get cross validation scores.\n",
    "* make a function that takes in x and y data, a list of models to test and number of folds of cross validation\n",
    "    * function should handle 3 models decision tree, knn, logistic regression \n",
    "    * use error handling to make sure the list contains selections from the above 3\n",
    "    * you can use set().issubset() to achieve this, set(input_models).issubet([\"dt\", \"knn\", \"lr\"]) for instance\n",
    "* you can use default params for the models\n",
    "* run cross validation for each model to be tested\n",
    "* once the best model is found, train a final model on all the data (best model as defined by CV)\n",
    "* this can be chosen using the best mean results from the CV score\n",
    "* return the results of the CV in a dictionary where the key is the model name the values are a list of cv scores\n",
    "* also return the best model, the actual model object not the name of the model \n",
    "* test on the iris dataset, using the 4 features and class labels\n",
    "* you can use SKlearn classes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "* make a list of 1,000,000,000 random numbers(np.random.randint should be of help) with the lowest value being 2 and the highest being 5\n",
    "* write a udf that will take a number and add 5 and return the nunmber\n",
    "* use the multiprocessing library to map this udf to the list, running 4 workers\n",
    "* put your code in a .py file in a text editor and run it using \"python file.py\" from the command line\n",
    "* paste the code in here (don't exectute it though)\n",
    "* when you run the script from the command line, print the runtime (you can use the time module to get the time at the start of the script and the end) to the console and take a screenshot of the console output and turn that in with your pdf notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "* run a decision tree regression on the boston dataset\n",
    "* create a training and test set\n",
    "* build the model on the train\n",
    "* and test it on the test set\n",
    "* use \"RAD\", \"ZN\", \"CRIM\", \"AGE\" as your features\n",
    "* print your training and test MSE and RMSE (note there is no rmse in sklearn so you will have to get the MSE and covert it somehow)\n",
    "* you can use Sklearn classes and functions here\n",
    "* round RMSE and MSE to 3 decimal places using round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "* use the iris dataset to make a decision tree classifier (on a training set)\n",
    "* use the trained model to make predictions for the test set\n",
    "* for the test set, create a dataframe that has actual class, pred class and pred probability for each class\n",
    "    * this will then have 5 columns (actual class, pred class, prob class 1, prob class 2, prob class 3)\n",
    "* create a confusion matrix and classification report for the model, using predicted and actual class values of the test set\n",
    "* make sure the confusion matrix is in a dataframe where the columns and index values are the class labels\n",
    "* you can use sklearn classes and functions here\n",
    "* model.classes_ will give you the ordering of the classes, since sklearn most of the time return index values of labels and not actual labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "* no need for a train and test on this one\n",
    "* use the boston housing dataset to create a regression model\n",
    "* use \"RAD\", \"ZN\", \"CRIM\", \"AGE\" as your features\n",
    "* print out the betas\n",
    "* now normalize the features to 0-1 scale.  \n",
    "* train another model using the normalized features\n",
    "* print out the new betas\n",
    "* make a dataframe that has the actual and predicted value for each observation for each model\n",
    "* manually make columns that has the Error, Squared Error, Root Squared Error and Absolute Error for each observation for each model\n",
    "    * so you will have a column \"mse\" and a column \"normalized_mse\" for example\n",
    "* show the descriptive statistics for the dataframe\n",
    "* you can use sklearn classes and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "* create a dataframe where the columns are words (tokens) and the rows sentences and the cells the count of each word\n",
    "* create a second dataframe where the cells are 1s and 0s, 1 if the the word is present, 0 if it isn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    \"this this is my first name\",\n",
    "    \"my first name is brian craft\",\n",
    "    \"there there once once was a dog with the name richard\",\n",
    "    \"once in a blue blue moon moon, a dog catches a fly\",\n",
    "    \"my first mate went to Europe Europe\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quesetion 9\n",
    "* What is something you could do with a dataframe like this?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
